
  Setp up ADF Pipelines to execute Azure Databricks Notebooks
  --------------------------------------------------------------------------
  
  Reference: https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables

  Prerequisites:
  -------------  
  - Existing Databricks workspace with notebooks that you want run using ADF pipeline. 
  - A "Shared access" all-purpose cluster is configured in your workspace.
  - Make sure you have completed the setup on your workspace. 
	- Make sure all the external locations, schemas are available in your workspace. 
	- Make sure the user has privileges on credential, external locations, catalog, schemas etc.
		- You can add all privileges to all account users to make it simple in your workspace
		- Open Catalog -> Permissions -> Grant -> Principals: All account users; Check: All Privileges
		  Open Schema -> Permissions -> Grant -> Principals: All account users; Check: All Privileges
		  Open Credential -> Permissions -> Grant -> Principals: All account users; Check: All Privileges
		  Open Ext. loc -> Permissions -> Grant -> Principals: All account users; Check: All Privileges
  - Make sure you have storage account with required datasets.
	  (I am using 'adfdatasa' as the storage account)

 -------------------------------------------------------------
  0. Prerequisite: Setup your storage account
	
	0.1 Create a storage account (adfdatasa)
	0.2 Create four containers in the storage account:
		- bookstore-landing, bookstore-bronze, bookstore-silver, bookstore-gold
	0.3 Upload the provided datasets to bookstore-landing container.
		This container should have three folders for three dates for which data is provided.
	0.4 Run the notebooks provided to setup external locations and schemas.
 -------------------------------------------------------------
 
 

 1. Create and open the Data Factory Service
 
	1.1 Search for and open 'Data factories' service.
	1.2 Click on 'Create' button and create a Data factory service as follows:
			Resource group: <your resource group>
			Name: ykrdemodf (select a unique name)
			Region: <your region>
			Version: V2
			-> Review + create -> Create
			Wait until the resource is deployed
			
	1.3 Go to the resource group page and open 'ykrdemodf' resource.
	1.4 Click on 'Launch Studio' button to open the data factory entry screen.	
	
 2. Add a role assignment to your data factory (ykrdemodf) on your databrick workspace.
 
	2.1 In your Azure portal, select your Databricks workspace (unitycatalog_ws in my case)
	2.2 Click on "Access control (IAM)" link
	2.3 Click on '+ Add' -> 'Add role assignment' option
	2.4 Under 'Privileged administrator roles' tab, select 'Contributor' role. Click 'Next'
	2.5 Select 'Managed identities', and add your data factory (i.e ykrdemodf) as a member.
	2.6 Click on 'Review + assign' -> 'Review + assign'  
	
 3. Create and setup a pipeline
 
	3.1 Click on "Author" (pencil icon) menu item.
	3.2 Click on "..." icon (context menu) of 'Pipelines' option and select "New pipeline"
	3.3 Setup your pipeline as follows:	
		Under "Properties" section, give pipeline a name. (Name: pl_bookstore_ingest)
		Under "Settings" tab, set concurrency to 1
			Concurrency: 1
		From the "Activities", expand the "Databricks" option and pull "Notebook" to the working pane.
		Click on 'General' tab and give the notebook activity a name (Name: Ingest books data)
	
 4. Create a 'Linked service' to your Databricks workspace.
  
	4.1 Select the 'Notebook' activity and under 'Azure Databricks' tab, click on 'New' to create a linked service.
		
	4.2 Create a linked service as follows:	
		Name: ls_unitycatalog_ws   
		Azure subscription: <select your subscription>
		Databricks workspace: unitycatalog_ws
		Authentication type: Managed server identity  (from step 2)
		Choose from existing clusters: <select your all-purpose cluster>
		Click on 'Test connection' to test
		Click on 'Create' to create a linked service.
				
  5. Select and attach the notebook to your activity.
  
	5.1 Select the 'Settings' tab
	5.2 Browse and select the notebook you want to run (2.1_ingest_books_data)
	-> At this point we can run the pipeline in debug mode (i.e without publishing the components)
	-> But we want to parameterise our pipeline
	
  6. Add parameters to your pipeline.
  
	6.1 Click on working pane outside your activity to go to pipeline settings.
	6.2 Click on 'Parameters' tab and add a parameter (remember this is pipeline parameter)
		Name: p_window_end_date   (this value will come from tumbling window trigger)
		Type: String
	6.3 Click on Databricks Notebook activity inside your working pane.
	6.4 Click on 'Settings' tab and under 'base parameters' click on '+ New'
	6.5 Add a parameter as follows:
		Name: date
		Value: @formatDateTime(pipeline().parameters.p_window_end_date, 'yyy-MM-dd')
		
  7. Test the pipeline using Debug
  
	7.1 Click on 'Debug' to run the pipeline
	7.2 Provide a valid date value (ex: 2025-12-01) to the parameter and run the pipeline.
	7.3 Check the results by clicking on the icons provided along side the pipeline run. 
	7.4 Click on 'Publish all' icon to publish the changes to your data factory.  
	
  8. Add other activities (for customers and orders notebooks)
  
	8.1 Right click on the "Ingest books data" activity and click copy.
	8.2 Right click and select 'Paste' twice to make two copies of it. 
	8.3 Change the name ("General" tab) and the notebook ("Settings") for these activities
		General 	=> Names: Ingest cutomers data; Ingest orders data
		Settings 	=> Notebooks: 2.2_ingest_customers_data, 2.3_ingest_orders_data
	8.4 You can run all these activities in parallel. 
	8.5 Test the pipeline by clicking on Debug.
	8.6 Click on 'Publish all' icon to publish the changes to your data factory.  
	
  9. Add an activity to check if your source data folder exists or not. 

	9.1 Expand 'General' under 'Activities' menu and drag 'Get Metadata' to your working pane. 
	9.2 Click on 'General' tab and give the activity a name 
		-> Name: Get folder details
	
	9.3 Click on 'Settings' tab and click on '+ New' to add a Dataset.		
		- Select "Azure Data Lake Storage gen2' and then select "Json' and click 'continue' button.
		(Here the format does not matter. We are check for a folder)
		- Give the dataset a name (Name: ds_bookstore_raw)
		- Under 'Linked service' dropdown, click '+ New' to create a linked service.
		This will send you to setup the linked service. 	
		
	9.4 Setup and create the linked service as follows:	
		Name: ls_bookstore_storage
		Authentication type: Account key
		Storage account name: Select your storage account (adfdatasa)
		Click in 'Test connection' and then click 'Create' to create the linked service	
		
	9.5 For 'File path', click on the browse icon and select 'bookstore-landing'. 
		We can not specify the 'directory' here.  This needs to come from runtime parameter. 
		
	9.6 Click on 'Advanced' and click on 'Open this dataset' link.	
	
	9.7 Setup the parameter for directoru as follows:
		- Click on 'Parameters' tab -> '+ New' -> Name: p_window_end_date
		- Click on 'Connection' tab
			- Click on 'File path' -> 'Directory' textbox. 
			- Click on 'Add dynamic content' link.
			- Add the following in the textbox:
			@formatDateTime(dataset().p_window_end_date, 'yyy-MM-dd')
			
	9.8 Click on your pipeline tab ('pl_bookstore_ingest' tab, near 'Publish all' button
	    Select the 'Get Metadata' activity.
		
		- Under 'Settings' tab, under 'Dataset properties', Click on 'Value' textbox
		- Add the following in the textbox:
			@pipeline().parameters.p_window_end_date
			(This is created by selecting 'Add dynamic content' and selecting the parameter)
		- Under 'Field list', Click on '+ New' and select 'Exists' from the dropdown. 
		
  10. Add and setup 'If Condition' activity to check and act based on 'exists' value of 'Get Metadata' activity.
  
	10.1 Expand 'Iteration and conditionals' and drag 'If Condition' activity to your working pane
	10.2 Link 'Get Metadata' activity to it by dragging success link and connecting it to 'If Condition'
	10.3 Click on 'If Condition' activity -> 'General' tab -> Name: 'If folder exists'
	10.4 Click on 'Activities' tab. Click on 'Expression' text box. Click on 'Add dynamic content' link
		- Click on 'Get folder details exists' option and click 'OK'.
		  The value should be: @activity('Get folder details').output.exists
		  
	** Now, if the above expression is true, then we want to execute all three of our notebook activities.
	   We will cut and paste the notebook activies under 'True' of the 'If Condition' activity.
	
	10.5 Select all three notebook activites and right click select 'Cut'
		Select the 'If Condition' activity, select 'Activities' tab and Click on pencil icon for the 'True' expression.
		This opens up a new page.
		Paste the notebooks here. (right click select 'paste')
		
  11. Test your pipeline now for both True and False conditions.
  
	11.1 Click on 'Debug' button and provide a non-existing folder as a parameter (ex: 2025-12-02)
		This should succeed this time, by not running the pipeline, as the 'exists' value evaluates to False.
		Click on output icon of pipeline run to check the details.
		
	
  12. Create a new pipeline for executing transformation notebooks (i.e silver and gold tables)
  
	12.1 Click on the "..." icon (context menu) of 'pl_bookstore_ingest' pipeline and click on 'Clone'
		This creates a clone of existing pipeline. 
		We need the 'p_window_end_date' parameter here as well, so we will keep it. 
		
	12.2 Under the Properties tab (right-side), change the name to 'pl_bookstore_transform'
	
	12.3 In the 'If Condition' activity, click on the 'pencil' icon under True. 
		This open the notebooks activities which are placed inside True condition. 
		
	12.4 Setup the notebooks for this pipeline. 
		- Keep one notebook and delete all other notebooks. 
		
		- Select the notebook activity
		- Under 'General' tab, change the name to 'Prepare orders cleaned'
		- Under Settings tab, browse and select '3_prepare_orders_cleaned' notebook
		- Under 'Azure Databricks', keep the linked service as it is. No changes required here.
		
		- Copy paste 'Prepare orders cleaned' notebook, to setup the next notebook.
		- Select the new notebook activity
		- Under 'General' tab, change the name to 'Report daily customer_books'
		- Under Settings tab, browse and select '4_report_daily_customer_books' notebook
		- Under 'Azure Databricks', keep the linked service as it is. No changes required here.		
	
	12.5 Setup the notebook dependencies.
		- Drag success link of 'Prepare orders cleaned' notebook and connect it to 'Report daily customer_books'
		- We want to run the second notebook after the first one. 
		
  13. Create a new pipeline to define the existing pipeline dependencies.

	13.1 Create a new pipeline  
		- Click on "..." icon in Pipelines menu item and select 'New pipeline'
		
	13.2 Under the Properties tab (right-side), change the name to 'pl_execute_pipelines' 
	13.2 Create a parameter called "p_window_end_date' under Parameters tab.
	13.3 Under 'Settings' tab, set 'Concurrency' to 1	
	13.4 Search for an drag 'Execute pipeline' activity to the working pane.
	13.5 Under 'General' tab, set the name to 'Ingestion pipeline'
	     Under 'Settings' tab, do the following:
		 Invoked pipeline: pl_bookstore_ingest
		 Parameters -> Value -> 'Add dynamic content' -> Select 'p_window_end_date' parameter
		   The value should look like: @pipeline().parameters.p_window_end_date
		 
	13.6 Search for an drag another 'Execute pipeline' activity to the working pane.
	13.7 Under 'General' tab, set the name to 'Transformation pipeline'
	     Under 'Settings' tab, do the following:
		 Invoked pipeline: pl_bookstore_transform
		 Parameters -> Value -> 'Add dynamic content' -> Select 'p_window_end_date' parameter
		   The value should look like: @pipeline().parameters.p_window_end_date
			
	13.8 Link the ingestion pipeline to the transformation pipeline 
		Drag success link and connect it to next pipeline activity. 
	
 14. Setup a trigger to execute our pipeline.
 
	14.1 Click on 'Manage' icon in the left menu. 
		Click on 'Author' -> 'Triggers' -> 'Create trigger' button
		
	14.2 Setup the trigger as follows:
	
		Name: tr_process_bookstore
		Type: Tumbling window
		Start date: 11/30/2025, 10:00:00 PM
		Recurrence: Every 24 hours
		End on: 12/5/2025, 10:00:00 PM
		Advanced -> Max concurrency: 1
		Start trigger: leave it checked.
		
	14.3 Add this trigger to your master pipeline (pl_execute_pipelines)
	
		Open the pl_execute_pipelines page
		Click on 'Add trigger' in the working pane menu and select 'New/Edit'
		Select your trigger
		Status: Started (change the status to 'started')
		Click OK
		Assign the following value to 'p_window_end_date': @trigger().outputs.windowEndTime
		Click OK.
		
	14.4 Click on 'Publish all' to publish the chages.
		This starts the trigger. 
		
	14.5 Click 'Monitor' -> 'Trigger runs' to monitor
		
  
			
		
		
	
		
		
	
	
	
  
	
	
	
	
	
	
	
	
	
	
	
	
	
	
  
		
	
		
	
	